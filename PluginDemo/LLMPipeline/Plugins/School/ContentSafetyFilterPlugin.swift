//
//  ContentSafetyFilterPlugin.swift
//  PluginDemo
//
//  Created by Tangorios on 2025/7/28.
//

import Foundation

/// å†…å®¹å®‰å…¨è¿‡æ»¤æ’ä»¶ - ç”¨äºå­¦æ ¡å®¢æˆ·çš„å†…å®¹å®‰å…¨æ£€æŸ¥
final class ContentSafetyFilterPlugin: LLMPlugin {
    let pluginId: String = "com.school.content_safety_filter"
    let priority: Int = 30
    
    // ä¸å½“å†…å®¹å…³é”®è¯
    private let inappropriateKeywords = [
        // æš´åŠ›ç›¸å…³
        "æš´åŠ›", "æ‰“æ¶", "ä¼¤å®³", "æ”»å‡»", "æ­¦å™¨",
        // ä¸å½“è¨€è®º
        "æ­§è§†", "ä»‡æ¨", "ç§æ—", "æ€§åˆ«æ­§è§†",
        // æˆäººå†…å®¹
        "è‰²æƒ…", "æ€§", "è£¸ä½“", "æˆäºº",
        // å±é™©è¡Œä¸º
        "è‡ªæ€", "è‡ªæ®‹", "æ¯’å“", "é…’ç²¾", "å¸çƒŸ",
        // å…¶ä»–ä¸å½“å†…å®¹
        "æ¬ºå‡Œ", "éœ¸å‡Œ", "ä½œå¼Š", "æŠ„è¢­"
    ]
    
    // ç§¯ææ­£é¢çš„æ›¿ä»£è¯æ±‡
    private let positiveAlternatives: [String: String] = [
        "æš´åŠ›": "å’Œå¹³è§£å†³å†²çª",
        "æ‰“æ¶": "å‹å¥½æ²Ÿé€š",
        "æ­§è§†": "åŒ…å®¹ç†è§£",
        "ä»‡æ¨": "å‹çˆ±äº’åŠ©",
        "æ¬ºå‡Œ": "å‹å–„ç›¸å¤„",
        "ä½œå¼Š": "è¯šä¿¡å­¦ä¹ "
    ]
    
    func processRequest(context: LLMContext) async throws {
        print("\(pluginId): Checking request content safety")
        
        let prompt = context.processedRequest.prompt.lowercased()
        var safetyIssues: [String] = []
        
        // æ£€æŸ¥ä¸å½“å†…å®¹
        for keyword in inappropriateKeywords {
            if prompt.contains(keyword) {
                safetyIssues.append(keyword)
            }
        }
        
        if !safetyIssues.isEmpty {
            print("\(pluginId): Safety issues detected: \(safetyIssues)")
            
            // è®°å½•å®‰å…¨é—®é¢˜
            context.sharedData["safety_issues"] = safetyIssues
            context.sharedData["content_filtered"] = true
            
            // æ ¹æ®å­¦æ ¡æ”¿ç­–å†³å®šæ˜¯å¦é˜»æ­¢è¯·æ±‚
            let userRole = context.sharedData["user_role"] as? String ?? "unknown"
            let grade = context.sharedData["grade"] as? String ?? "unknown"
            
            if shouldBlockContent(issues: safetyIssues, userRole: userRole, grade: grade) {
                let errorMessage = "Content contains inappropriate material for educational environment: \(safetyIssues.joined(separator: ", "))"
                throw LLMPipelineError.contentViolation(errorMessage)
            } else {
                // ä¸é˜»æ­¢ä½†æ·»åŠ è­¦å‘Š
                context.sharedData["content_warning"] = true
                print("\(pluginId): Content warning added, but request allowed to proceed")
            }
        }
        
        // æ£€æŸ¥æ˜¯å¦åŒ…å«å­¦æœ¯è¯šä¿¡ç›¸å…³é—®é¢˜
        checkAcademicIntegrity(context: context)
        
        print("\(pluginId): Request content safety check completed")
    }
    
    func processResponse(context: LLMContext) async throws {
        print("\(pluginId): Filtering response content")
        
        guard let response = context.finalResponse else {
            print("\(pluginId): No response to filter")
            return
        }
        
        var filteredContent = response.content
        var contentModified = false
        
        // è¿‡æ»¤å“åº”ä¸­çš„ä¸å½“å†…å®¹
        for keyword in inappropriateKeywords {
            if filteredContent.lowercased().contains(keyword) {
                if let alternative = positiveAlternatives[keyword] {
                    filteredContent = filteredContent.replacingOccurrences(
                        of: keyword,
                        with: alternative,
                        options: .caseInsensitive
                    )
                    contentModified = true
                    print("\(pluginId): Replaced '\(keyword)' with '\(alternative)'")
                }
            }
        }
        
        // æ·»åŠ æ•™è‚²æ€§çš„å®‰å…¨æé†’
        if context.sharedData["content_warning"] as? Bool == true {
            let safetyReminder = generateSafetyReminder(context: context)
            filteredContent = filteredContent + "\n\n" + safetyReminder
            contentModified = true
        }
        
        // æ›´æ–°å“åº”
        if contentModified {
            var updatedMetadata = response.metadata
            updatedMetadata["content_filtered"] = true
            updatedMetadata["safety_filter_applied"] = true
            
            context.finalResponse = LLMResponse(
                content: filteredContent,
                metadata: updatedMetadata,
                processingTime: response.processingTime
            )
            
            print("\(pluginId): Response content filtered and safety reminders added")
        } else {
            print("\(pluginId): No content filtering needed")
        }
    }
    
    private func shouldBlockContent(issues: [String], userRole: String, grade: String) -> Bool {
        // å¯¹äºé«˜ä¸­ä»¥ä¸‹å­¦ç”Ÿï¼Œæ›´ä¸¥æ ¼çš„è¿‡æ»¤
        if grade == "high_school" || grade == "middle_school" || grade == "elementary" {
            return !issues.isEmpty
        }
        
        // å¯¹äºå¤§å­¦ç”Ÿå’Œæ•™å¸ˆï¼Œåªé˜»æ­¢ä¸¥é‡çš„ä¸å½“å†…å®¹
        let seriousIssues = ["æš´åŠ›", "è‰²æƒ…", "è‡ªæ€", "æ¯’å“"]
        return issues.contains { seriousIssues.contains($0) }
    }
    
    private func checkAcademicIntegrity(context: LLMContext) {
        let prompt = context.processedRequest.prompt.lowercased()
        
        // æ£€æŸ¥å¯èƒ½çš„å­¦æœ¯ä¸è¯šä¿¡è¡Œä¸º
        let academicIntegrityKeywords = [
            "å¸®æˆ‘å†™ä½œä¸š", "ä»£å†™", "ç­”æ¡ˆæ˜¯ä»€ä¹ˆ", "ç›´æ¥ç»™æˆ‘ç­”æ¡ˆ",
            "è€ƒè¯•ç­”æ¡ˆ", "ä½œä¸šç­”æ¡ˆ", "å¸®æˆ‘å®Œæˆ", "æ›¿æˆ‘åš"
        ]
        
        for keyword in academicIntegrityKeywords {
            if prompt.contains(keyword) {
                context.sharedData["academic_integrity_concern"] = true
                context.sharedData["integrity_keyword"] = keyword
                print("\(pluginId): Academic integrity concern detected: \(keyword)")
                break
            }
        }
    }
    
    private func generateSafetyReminder(context: LLMContext) -> String {
        let userRole = context.sharedData["user_role"] as? String ?? "student"
        let grade = context.sharedData["grade"] as? String ?? "unknown"
        
        var reminder = "ğŸ›¡ï¸ å®‰å…¨æé†’ï¼š"
        
        if let academicConcern = context.sharedData["academic_integrity_concern"] as? Bool, academicConcern {
            reminder += """
            
            
            ğŸ“š å­¦æœ¯è¯šä¿¡æé†’ï¼š
            â€¢ å­¦ä¹ çš„ç›®çš„æ˜¯ç†è§£å’ŒæŒæ¡çŸ¥è¯†ï¼Œè€Œä¸æ˜¯è·å¾—ç­”æ¡ˆ
            â€¢ ç‹¬ç«‹æ€è€ƒå’Œè§£å†³é—®é¢˜æ˜¯é‡è¦çš„å­¦ä¹ æŠ€èƒ½
            â€¢ å¦‚éœ€å¸®åŠ©ï¼Œè¯·å¯»æ±‚è€å¸ˆæˆ–åŒå­¦çš„æŒ‡å¯¼ï¼Œè€Œéç›´æ¥ç­”æ¡ˆ
            â€¢ è¯šä¿¡æ˜¯å­¦æœ¯ç ”ç©¶å’Œä¸ªäººå‘å±•çš„åŸºçŸ³
            """
        }
        
        if let safetyIssues = context.sharedData["safety_issues"] as? [String], !safetyIssues.isEmpty {
            reminder += """
            
            
            ğŸŒŸ æ­£é¢å¼•å¯¼ï¼š
            â€¢ é‡åˆ°å›°éš¾æ—¶ï¼Œå¯»æ±‚ç§¯ææ­£é¢çš„è§£å†³æ–¹æ¡ˆ
            â€¢ ä¸è€å¸ˆã€å®¶é•¿æˆ–å¿ƒç†å’¨è¯¢å¸ˆäº¤æµ
            â€¢ åŸ¹å…»å¥åº·çš„å…´è¶£çˆ±å¥½å’Œç¤¾äº¤å…³ç³»
            â€¢ è®°ä½ï¼šæ¯ä¸ªé—®é¢˜éƒ½æœ‰ç§¯æçš„è§£å†³æ–¹å¼
            """
        }
        
        // æ ¹æ®å¹´çº§æ·»åŠ ç‰¹å®šæé†’
        switch grade {
        case "high_school":
            reminder += """
            
            
            ğŸ“ é«˜ä¸­ç”Ÿç‰¹åˆ«æé†’ï¼š
            â€¢ ä¸“æ³¨äºå­¦ä¸šå’Œä¸ªäººå‘å±•
            â€¢ åŸ¹å…»æ‰¹åˆ¤æ€§æ€ç»´å’Œç‹¬ç«‹åˆ¤æ–­èƒ½åŠ›
            â€¢ å¦‚æœ‰å¿ƒç†å‹åŠ›ï¼ŒåŠæ—¶å¯»æ±‚å¸®åŠ©
            """
        case "undergraduate":
            reminder += """
            
            
            ğŸ“ å¤§å­¦ç”Ÿç‰¹åˆ«æé†’ï¼š
            â€¢ ä¿æŒå­¦æœ¯è¯šä¿¡å’Œé“å¾·æ ‡å‡†
            â€¢ å‘å±•ä¸“ä¸šæŠ€èƒ½å’Œç¤¾ä¼šè´£ä»»æ„Ÿ
            â€¢ å»ºç«‹å¥åº·çš„å­¦ä¹ å’Œç”Ÿæ´»ä¹ æƒ¯
            """
        default:
            break
        }
        
        return reminder
    }
    
    func audit(context: LLMContext) async {
        print("\(pluginId): Recording content safety audit")
        
        let safetyAudit = ContentSafetyAudit(
            requestId: context.originalRequest.requestId,
            userId: context.sharedData["user_id"] as? String ?? "unknown",
            userRole: context.sharedData["user_role"] as? String ?? "unknown",
            safetyIssues: context.sharedData["safety_issues"] as? [String] ?? [],
            contentFiltered: context.sharedData["content_filtered"] as? Bool ?? false,
            academicIntegrityConcern: context.sharedData["academic_integrity_concern"] as? Bool ?? false,
            timestamp: Date()
        )
        
        // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›å®¡è®¡è®°å½•ä¼šå­˜å‚¨åˆ°å®‰å…¨æ•°æ®åº“ä¸­
        print("\(pluginId): Safety audit - Issues: \(safetyAudit.safetyIssues.count), Filtered: \(safetyAudit.contentFiltered)")
        
        if safetyAudit.academicIntegrityConcern {
            print("\(pluginId): Academic integrity concern logged for user \(safetyAudit.userId)")
        }
    }
}

// MARK: - Content Safety Audit Model

struct ContentSafetyAudit {
    let requestId: String
    let userId: String
    let userRole: String
    let safetyIssues: [String]
    let contentFiltered: Bool
    let academicIntegrityConcern: Bool
    let timestamp: Date
}
